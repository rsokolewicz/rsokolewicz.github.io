<!doctype html><html lang=en><head><title>Easy data-scraping using REST API and request package :: Roberts blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Often when webpages provide a friendly interface to download data, the data itself is fetched using a REST API. Most web browsers can easily intercept the request, which you can modify and edit. This is convenient if you want to use Python to download a lot of data, or if you want to set up an automated data download pipeline.
Here&amp;rsquo;s a quick step-by-step guide on how to do so."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://www.robert-sokolewicz.nl/posts/6_rest_api/><link rel=stylesheet href=https://www.robert-sokolewicz.nl/assets/style.css><link rel=stylesheet href=https://www.robert-sokolewicz.nl/assets/css/hugo-cite.css><link rel=apple-touch-icon href=https://www.robert-sokolewicz.nl/img/favicon/apple-touch.png><link rel="shortcut icon" href=https://www.robert-sokolewicz.nl/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Easy data-scraping using REST API and request package"><meta property="og:description" content="Often when webpages provide a friendly interface to download data, the data itself is fetched using a REST API. Most web browsers can easily intercept the request, which you can modify and edit. This is convenient if you want to use Python to download a lot of data, or if you want to set up an automated data download pipeline.
Here&amp;rsquo;s a quick step-by-step guide on how to do so."><meta property="og:url" content="https://www.robert-sokolewicz.nl/posts/6_rest_api/"><meta property="og:site_name" content="Roberts blog"><meta property="og:image" content="https://www.robert-sokolewicz.nl/img/favicon/orange.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2023-04-10 00:00:00 +0000 UTC"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],macros:{bb:["{\\boldsymbol{#1}}",1],tr:"{\\DeclareMathOperator{\\tr}{Tr}}",im:"{\\DeclareMathOperator{\\im}{Im}}",re:"{\\DeclareMathOperator{\\re}{Re}}",},processEscapes:true,processEnvironments:true,tags:"ams"},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
    MathJax.Hub.Queue(function() {
        // Fix <code> tags after MathJax finishes running. This is a
        // hack to overcome a shortcoming of Markdown. Discussion at
        // https://github.com/mojombo/jekyll/issues/199
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i &lt; all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Home</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/books>Book Reviews</a></li><li><a href=/cv>CV</a></li><li><a href=/physics>Physics</a></li><li><a href=/reading_papers>Reading Papers</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/books>Book Reviews</a></li><li><a href=/cv>CV</a></li><li><a href=/physics>Physics</a></li><li><a href=/reading_papers>Reading Papers</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://www.robert-sokolewicz.nl/posts/6_rest_api/>Easy data-scraping using REST API and request package</a></h1><div class=post-meta><span class=post-date>2023-04-10</span>
<span class=post-author>:: Robert</span></div><div class=post-content><div><p>Often when webpages provide a friendly interface to download data, the data itself is fetched using a REST API. Most web browsers can easily intercept the request, which you can modify and edit. This is convenient if you want to use Python to download a lot of data, or if you want to set up an automated data download pipeline.</p><p>Here&rsquo;s a quick step-by-step guide on how to do so.</p><h2 id=step-1-inspect-the-web-page>Step 1: Inspect the Web Page<a href=#step-1-inspect-the-web-page class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Open your web browser and navigate to the website from which you want to download data. For this example, I use <a href=https://daggegevens.knmi.nl/klimatologie/daggegevens>weather data</a> from the Dutch Royal Weather Institute (KNMI).
Right-click on the web page and select &ldquo;Inspect&rdquo; or &ldquo;Inspect Element&rdquo; from the context menu. This will open the browser&rsquo;s developer tools.</p><figure class=center style="margin:0 auto"><img src=images/image-4.png style="display:block;margin:0 auto"></figure><p>In the developer tools, go to the &ldquo;Network&rdquo; tab. This tab allows you to monitor the network activity of the web page, including the REST API requests and responses.</p><figure class=center style="margin:0 auto"><img src=images/image-5.png style="display:block;margin:0 auto"></figure><h2 id=step-2-identify-the-rest-api-endpoint>Step 2: Identify the REST API Endpoint<a href=#step-2-identify-the-rest-api-endpoint class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Interact with the web page to trigger the REST API request that fetches the data you want to download. In the KNMI data example, we can enter:</p><ul><li>Period: 20230101 - 20230410</li><li>Fields: TG (daily mean temperature)</li><li>Weather stations: 260 de Bilt</li></ul><p>and press &ldquo;Bestand downloaden&rdquo;. Most webpages should have either a similar download button or a search query.</p><p>In the &ldquo;Network&rdquo; tab, you should see a list of network requests made by the web page. Look for the REST API request that corresponds to the data you want to download. The REST API request will usually have a URL, method (e.g., GET, POST), and request/response headers. In this example, we look at &ldquo;daggegevens&rdquo;.</p><figure class=center style="margin:0 auto"><img src=images/image-6.png style="display:block;margin:0 auto"></figure><h2 id=step-3-download-the-rest-api-request-as-curl>Step 3: Download the REST API Request as cURL<a href=#step-3-download-the-rest-api-request-as-curl class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Right-click the REST API request that you want to download and select &ldquo;Copy&rdquo; > &ldquo;Copy all as cURL&rdquo; from the context menu. This will copy the cURL command that represents the REST API request to your clipboard.</p><figure class=center style="margin:0 auto"><img src=images/image-7.png style="display:block;margin:0 auto"></figure><p>Open a text editor or a terminal window, and paste the copied cURL command.
Modify the cURL command as needed, such as replacing placeholders with actual values. For example, you may need to update query parameters, request headers, or authentication tokens.
Once you have configured the cURL command, you can execute it in your terminal to download the data from the REST API endpoint. The downloaded data will typically be saved as a response body in the format specified by the REST API (e.g., JSON, XML, CSV).</p><h2 id=step-4-write-a-python-script-for-automated-download>Step 4: Write a Python script for automated download<a href=#step-4-write-a-python-script-for-automated-download class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Instead of manually modifying the cURL command and running it from the terminal, it is convenient to do this in Python. This will allow us to automatically sweep through parameters and download everything that we need.</p><p>We first convert the cURL request that is sitting in our clipboard, to a Python script that will use the requests package. A convenient webpage is <a href=https://curlconverter.com/python/>curlconverter.com/python/</a>.</p><figure class=center style="margin:0 auto"><img src=images/image-8.png style="display:block;margin:0 auto"></figure><p>You should have a script that is similar to</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> requests

cookies <span style=color:#f92672>=</span> {
    <span style=color:#f92672>...</span>
}

headers <span style=color:#f92672>=</span> {
    <span style=color:#f92672>...</span>
}

data <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;...&#39;</span>

response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(<span style=color:#e6db74>&#39;...&#39;</span>, cookies<span style=color:#f92672>=</span>cookies, headers<span style=color:#f92672>=</span>headers, data<span style=color:#f92672>=</span>data)
</code></pre></div><p>In the example of the KNMI weather data, a single download is limited to about 2 years of data. If we want to build a dataset that contains full historical data, we can loop over all the years that we are interested in, where we iteratively run <code>requests.post(...)</code> but for the correct years and append it to a larger Pandas <code>DataFrame</code>. Inside the large data string we find <code>name="start"\r\n\r\n20230101\r\n</code> and <code>name="end"\r\n\r\n20230410\r\n</code>, so we insert variables here over which we can loop:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>data <span style=color:#f92672>=</span> f<span style=color:#e6db74>&#39;... name=&#34;start&#34;</span><span style=color:#ae81ff>\r\n\r\n</span><span style=color:#e6db74>{year}0101</span><span style=color:#ae81ff>\r\n</span><span style=color:#e6db74> ... name=&#34;end&#34;</span><span style=color:#ae81ff>\r\n\r\n</span><span style=color:#e6db74>{year+2}1231</span><span style=color:#ae81ff>\r\n</span><span style=color:#e6db74> ...&#39;</span>
</code></pre></div><p>and the looping script will look as:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
<span style=color:#f92672>from</span> tqdm <span style=color:#f92672>import</span> tqdm
<span style=color:#f92672>from</span> io <span style=color:#f92672>import</span> BytesIO

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_data</span>(year):
    data <span style=color:#f92672>=</span> f<span style=color:#e6db74>&#39;... name=&#34;start&#34;</span><span style=color:#ae81ff>\r\n\r\n</span><span style=color:#e6db74>{year}0101</span><span style=color:#ae81ff>\r\n</span><span style=color:#e6db74> ... name=&#34;end&#34;</span><span style=color:#ae81ff>\r\n\r\n</span><span style=color:#e6db74>{year+2}1231</span><span style=color:#ae81ff>\r\n</span><span style=color:#e6db74> ...&#39;</span>
    <span style=color:#66d9ef>return</span> data

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_chunked_df</span>(year):
    data <span style=color:#f92672>=</span> get_data(year)

    response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(
        <span style=color:#e6db74>&#34;https://daggegevens.knmi.nl/klimatologie/daggegevens&#34;</span>,
        cookies<span style=color:#f92672>=</span>cookies,
        headers<span style=color:#f92672>=</span>headers,
        data<span style=color:#f92672>=</span>data,
    )

    <span style=color:#66d9ef>if</span> response<span style=color:#f92672>.</span>status_code <span style=color:#f92672>==</span> <span style=color:#ae81ff>200</span>:
        content <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>content
        <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>read_csv(BytesIO(content), skiprows<span style=color:#f92672>=</span><span style=color:#ae81ff>60</span>)
    <span style=color:#66d9ef>else</span>:
        <span style=color:#66d9ef>return</span> None


<span style=color:#66d9ef>for</span> year <span style=color:#f92672>in</span> tqdm(range(<span style=color:#ae81ff>1900</span>, <span style=color:#ae81ff>2022</span>, <span style=color:#ae81ff>2</span>)):
    df_chunk <span style=color:#f92672>=</span> get_chunked_df(year)
    df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat([df, df_chunk])

df<span style=color:#f92672>.</span>to_csv(<span style=color:#e6db74>&#34;data/weather.csv&#34;</span>, index<span style=color:#f92672>=</span>False)
</code></pre></div><p>some notes about this code:</p><ul><li>an HTTP request always returns a <code>status_code</code>. <code>200</code> means you made a successful request and any other code indicates that something went wrong (e.g. <code>403</code> - access denied).</li><li><code>response.content</code> returns a <em>byte stream</em> that contains the content of the <code>.csv</code> file that we requested. This can be passed directly to <code>pandas.read_csv</code> using <code>io.BytesIO</code>, so no need to save the file to disk first.</li><li>we pass the argument <code>skiprows=60</code> to <code>.read_csv(...)</code> because the KNMI data contains 60 rows of comments with explanations of what each data field means. The actual data that we are interested in starts from line <code>61</code>.</li></ul><p>That&rsquo;s it! We have now successfully downloaded data from a website using REST API by inspecting the web page, identifying the REST API endpoint, and downloading the REST API request as cURL. Remember to always review and follow the terms of use and policies of the website and REST API you are accessing, and be respectful of any usage limits or restrictions. Frequent and bulky downloads are not always appreciated and you might get a temporary IP ban if you overuse the API endpoint.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://www.robert-sokolewicz.nl/posts/7_mock_context_manager/><span class=button__icon>←</span>
<span class=button__text>Mocking context managers with pytest</span></a></span>
<span class="button next"><a href=https://www.robert-sokolewicz.nl/posts/3_vs_code_tips/><span class=button__text>Some tips for using VS code for python development</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2025 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://www.robert-sokolewicz.nl/assets/main.js></script><script src=https://www.robert-sokolewicz.nl/assets/prism.js></script></div></body></html>