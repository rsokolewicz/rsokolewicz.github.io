<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Roberts blog</title>
    <link>https://rsokolewicz.github.io/posts/</link>
    <description>Recent content in Posts on Roberts blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://rsokolewicz.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Query Likelihood Model</title>
      <link>https://rsokolewicz.github.io/posts/2_query_likelihood/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rsokolewicz.github.io/posts/2_query_likelihood/</guid>
      <description>Language Models One of the central ideas behind language modeling is that when a user tries to produce a good search query, he or she will come up with terms that are likely to appear in a relevant document. In other words, a relevant document is one that is likely to contain the query terms. What makes language modeling different from other probabilistic models, is that it creates a language model for each document from which probabilities are generated that correspond to the likelihood that a query can be found in that document.</description>
      <content>&lt;h1 id=&#34;language-models&#34;&gt;Language Models&lt;/h1&gt;
&lt;p&gt;One of the central ideas behind language modeling is that when a user tries to produce a good search query, he or she will come up with terms that are likely to appear in a relevant document. In other words, a relevant document is one that is likely to contain the query terms. What makes language modeling different from other probabilistic models, is that it creates a language model for each document from which probabilities are generated that correspond to the likelihood that a query can be found in that document. This probability is given by $P(q|M_d)$.&lt;/p&gt;
&lt;p&gt;The definition of a language model is a function that produces probabilities for a word or collection of words (e.g. a (part of a) sentence) given a vocabulary. Let us look at an example of a model that produces probabilities for single words:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;s&lt;/th&gt;
&lt;th&gt;cat&lt;/th&gt;
&lt;th&gt;dog&lt;/th&gt;
&lt;th&gt;likes&lt;/th&gt;
&lt;th&gt;fish&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$P(s)$&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The probability for the sentence &amp;ldquo;cat likes fish&amp;rdquo; is $0.3\times0.2\times0.2 = 0.012$, whereas the probability for the sentence &amp;ldquo;dog likes cat&amp;rdquo; is $0.1\times0.2\times0.3 = 0.006$. This means that the term &amp;ldquo;cat likes fish&amp;rdquo; is more likely to appear in the document than &amp;ldquo;dog likes cat&amp;rdquo;. If we want to compare different documents with the same search query, we produce the probability for each document separately. Remember that each document has its own language model with different probabilities.&lt;/p&gt;
&lt;p&gt;Another way of interpreting these probabilities is asking how likely it is that this model generates the sentence &amp;ldquo;cat likes fish&amp;rdquo; or &amp;ldquo;dog likes cat&amp;rdquo;. (Technically speaking you should also include probabilities how likely it is that a sentence continues or stops after each word). These sentences don&amp;rsquo;t have to exist in the document, nor do they have to make sense. In this language model for example, the sentences &amp;ldquo;cat likes fish&amp;rdquo; and &amp;ldquo;cat fish fish&amp;rdquo; have the same probability, in other words they are equally likely to be generated.&lt;/p&gt;
&lt;p&gt;The language model from the example above is called a unigram language model, it is a model that estimates each term independently and ignores the context. One language model that does include context is the bigram language model. This model includes conditional probabilities for terms given that they are preceded by another term. The probability for &amp;ldquo;cat likes fish&amp;rdquo; would be given by&lt;/p&gt;
&lt;p&gt;$$ P(\text{cat}) \times P(\text{likes}|\text{cat}) \times P(\text{fish}|\text{likes}). $$&lt;/p&gt;
&lt;p&gt;This of course requires all conditional probabilities to exist.&lt;/p&gt;
&lt;p&gt;More complex models exist, but they are less likely to be used. Each document creates a new language model, but the training data within one document is often not sufficiently large enough to accurately train a more complex model. This is reminiscent of the bias-variance trade-off. Complex models have high variance and are prone to overfitting on smaller training data.&lt;/p&gt;
&lt;h1 id=&#34;the-query-likelihood-model&#34;&gt;The Query Likelihood Model&lt;/h1&gt;
&lt;p&gt;When ranking documents by how relevant they are to a query, we are interested in the conditional probability $P(d|q)$. In the query likelihood model, this probability is so-called rank-equivalent to $P(q|d)$, so that we only need to use the probabilities discussed above. To see why they are rank-equivalent let us look at Bayes Rule:&lt;/p&gt;
&lt;p&gt;$$ P(d|q) = P(q|d) P(d) / P(q) $$&lt;/p&gt;
&lt;p&gt;Since $P(q)$ has the same value for each document, it will not affect the ranking at all. $P(d)$ on the other hand is treated as being uniform for simplicity and so will not affect the ranking either (in more complicated models $P(d)$ could be made dependent on the length of the document for example). And so, the probability $P(d|q)$ is equivalent to $P(q|d)$. In other words, in the query likelihood model the following two statements are &lt;em&gt;rank-equivalent&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The likelihood that document d is relevant to query q.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability that query q is generated by the language of document d.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a user creates a query, he or she already has an idea of how a relevant document could look like. The terms used in the query are more likely to appear in relevant documents than in non-relevant documents.&lt;/p&gt;
&lt;p&gt;One way of estimating the probability $P(q|d)$ for a unigram model is using the maximum likelihood estimation&lt;/p&gt;
&lt;p&gt;$$ P(Q|M_d) = \Pi_{t\in q} P_{\text{mle}}(T|M_d) = \Pi_{t\in q}\frac{\text{tf}_{t,d}}{L_d}$$&lt;/p&gt;
&lt;p&gt;Where $\text{tf}_{t,d}$ is the term frequency of term $t$ in document $d$ and $L_d$ is the size of document $d$. In other words, calculate the fraction of how often each query word appears in document $d$ compared to all words in that document, and then multiply all those fractions with each other.&lt;/p&gt;
&lt;p&gt;There are two small problems with the formula above. First, if one the terms in the query does not appear in a document, the entire probability $P(q|d)$ will be zero. In other words, the only way to get a non-zero probability is if each term in the query appears in the document. The second problem is that the probability of the terms that appear less frequently in the document are likely to be overestimated.&lt;/p&gt;
&lt;p&gt;The solution to these problems is to introduce smoothing. Smoothing will help by creating non-zero probabilities for terms that do not appear in the document, and by creating effective weights to frequent terms. Different smoothing techniques exist such as Jelinek-Mercer smoothing, that uses a linear combination of document-specific and collection-specific maximum likelihood estimations&lt;/p&gt;
&lt;p&gt;$$ P(T|d) = \lambda P_\text{mle} (t|M_d) +(1-\lambda)P_\text{mle}(t|M_c) $$&lt;/p&gt;
&lt;p&gt;where $0&amp;lt;\lambda&amp;lt;1$ is a hyperparameter that can be finetuned and $M_c$ is a language model created on the entire document collection. Another popular smoothing technique is Dirichlet smoothing&lt;/p&gt;
&lt;p&gt;$$ P(t|d) = \frac{\text{tf}_{t,d}+\alpha P(t|M_c)}{L_d + \alpha} $$&lt;/p&gt;
&lt;p&gt;(parts of this article also appear in this &lt;a href=&#34;https://medium.com/towards-data-science/understanding-term-based-retrieval-methods-in-information-retrieval-2be5eb3dde9f&#34;&gt;Medium post&lt;/a&gt; about term-based retrieval methods in information retrieval I wrote with my gf)&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Copy large amounts of files over ssh</title>
      <link>https://rsokolewicz.github.io/posts/1_copy_large_amounts_of_files/</link>
      <pubDate>Wed, 13 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rsokolewicz.github.io/posts/1_copy_large_amounts_of_files/</guid>
      <description>To copy large amounts of files over ssh you have a few options. The slowest option is using scp -r to recursively copy each file one by one, but it is faster and more convenient to first tar or gzip your home directory, copy it over to your host computer and unpack it there. This is possible to do in a single line
tar czf - &amp;lt;files&amp;gt; | ssh user@host &amp;quot;tar -C &amp;lt;destination&amp;gt; xvzf -&amp;quot; where &amp;lt;files&amp;gt; are paths to the files and directories to tar and &amp;lt;destination&amp;gt; is the destination folder on the host computer.</description>
      <content>&lt;p&gt;To copy large amounts of files over ssh you have a few options. The slowest option is using &lt;code&gt;scp -r&lt;/code&gt; to recursively copy each file one by one, but it is faster and more convenient to first tar or gzip your home directory, copy it over to your host computer and unpack it there. This is possible to do in a single line&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34; data-lang=&#34;{bash}&#34;&gt;tar czf - &amp;lt;files&amp;gt; | ssh user@host &amp;quot;tar -C &amp;lt;destination&amp;gt; xvzf -&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where &lt;code&gt;&amp;lt;files&amp;gt;&lt;/code&gt; are paths to the files and directories to tar and &lt;code&gt;&amp;lt;destination&amp;gt;&lt;/code&gt; is the destination folder on the host computer. One downside of this approach is that you won&amp;rsquo;t have a clear indication on how long it will take. An alternative approach is to first tar everything, and then use &lt;code&gt;rsync&lt;/code&gt; to send everything over.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34; data-lang=&#34;{bash}&#34;&gt;tar czf &amp;lt;files&amp;gt;
rsync -av --progress /foo/*.tar.gz user@host:&amp;lt;directory&amp;gt;

```&lt;/code&gt;&lt;/pre&gt;</content>
    </item>
    
    <item>
      <title>How to ssh over WiFi between two machines running wsl</title>
      <link>https://rsokolewicz.github.io/posts/0_ssh_over_wifi_wsl/</link>
      <pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rsokolewicz.github.io/posts/0_ssh_over_wifi_wsl/</guid>
      <description>We first follow a tutorial by Scott Hanselman where we will use Windows&amp;rsquo; openSSH to handle ssh connections and set the default ssh shell to be bash on wsl.
Setup ssh on your machine Choose one of your two machines to be the host. On the host, we need to first check if OpenSSH.Server is installed. Open powershell with elevated rights and run the following
&amp;gt; Get-WindowsCapability -Online | ? Name -like &#39;OpenSSH*&#39; Name : OpenSSH.</description>
      <content>&lt;p&gt;We first follow a tutorial by &lt;a href=&#34;https://www.hanselman.com/blog/the-easy-way-how-to-ssh-into-bash-and-wsl2-on-windows-10-from-an-external-machine&#34;&gt;Scott Hanselman&lt;/a&gt; where we will use Windows&amp;rsquo; openSSH to handle ssh connections and set the default ssh shell to be bash on wsl.&lt;/p&gt;
&lt;h2 id=&#34;setup-ssh-on-your-machine&#34;&gt;Setup ssh on your machine&lt;/h2&gt;
&lt;p&gt;Choose one of your two machines to be the host. On the host, we need to first check if OpenSSH.Server is installed. Open powershell with elevated rights and run the following&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{cmd}&#34; data-lang=&#34;{cmd}&#34;&gt;&amp;gt; Get-WindowsCapability -Online | ? Name -like &#39;OpenSSH*&#39;

Name  : OpenSSH.Client~~~~0.0.1.0
State : Installed

Name  : OpenSSH.Server~~~~0.0.1.0
State : NotPresent
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If the state under OpenSSH.server is &lt;code&gt;NotPresent&lt;/code&gt;, we need to run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{cmd]&#34; data-lang=&#34;{cmd]&#34;&gt;Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and start the ssh daemon&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{cmd}&#34; data-lang=&#34;{cmd}&#34;&gt;Start-Service sshd
Get-Service sshd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you expect to use the Host more often, you can consider starting the ssh daemon automatically on the host&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{cmd}&#34; data-lang=&#34;{cmd}&#34;&gt;Set-Service -Name sshd -StartupType &#39;Automatic&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we will set the default shell used by OpenSSH to be the one used by wsl&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{cmd}&#34; data-lang=&#34;{cmd}&#34;&gt;New-ItemProperty -Path &amp;quot;HKLM:\SOFTWARE\OpenSSH&amp;quot; -Name DefaultShell -Value &amp;quot;C:\WINDOWS\System32\bash.exe&amp;quot; -PropertyType String -Force
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;make-your-machine-discoverable-over-network&#34;&gt;Make your machine discoverable over network&lt;/h2&gt;
&lt;p&gt;To be able to find your host machine that runs the ssh daemon, you need to make some configurations in the network settings first. On both machines, go to network settings and under &amp;ldquo;network profile&amp;rdquo;, select &amp;ldquo;private network&amp;rdquo;. This will allow both machines to be discoverable on the WiFi network. If all is well, when you open File Explorer, under Network you should see your two machines. If you don&amp;rsquo;t see it, you might need to restart your computer(s), or temporary turn off Windows firewall.&lt;/p&gt;
&lt;h2 id=&#34;preparing-to-copy&#34;&gt;Preparing to copy&lt;/h2&gt;
&lt;p&gt;Now that this is all set-up, you should be able to ssh into your host computer via&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{bash}&#34; data-lang=&#34;{bash}&#34;&gt;ssh user@host
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;where &lt;code&gt;user&lt;/code&gt; is your Windows login username (not your wsl linux username), and &lt;code&gt;host&lt;/code&gt; is your host computer&amp;rsquo;s local private ip address that looks like &lt;code&gt;192.168.x.x&lt;/code&gt; or &lt;code&gt;172.x.x.x&lt;/code&gt;. You can get the ip address by either running &lt;code&gt;ifconfig eth0&lt;/code&gt; in your terminal, or by opening your Windows Network Settings (win-key &amp;gt; &amp;ldquo;network settings&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;If you do not manage to connect, make sure the ssh daemon is running on your host computer by running &lt;code&gt;sudo service ssh status&lt;/code&gt;. If it is not running you can start it with &lt;code&gt;sudo service ssh start&lt;/code&gt;. If you still cannot connect, try to open port 22 in your firewall on the host computer, or temporary turn off Windows firewall.&lt;/p&gt;
&lt;p&gt;If all is correct you should get inside your host&amp;rsquo;s wsl partition directly.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>elements</title>
      <link>https://rsokolewicz.github.io/posts/1_elements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rsokolewicz.github.io/posts/1_elements/</guid>
      <description>Table of contents  Table of contents Text Code  Python C/C++ Bash   Tables  Text Some example text
Code Python def foo(): print (&amp;#34;This is a python function&amp;#34;) C/C++ void foo(){ prinf(&amp;#34;%s\n&amp;#34;, &amp;#34;This is a C function&amp;#34;) } Bash # This is a bash command cd dir &amp;amp;&amp;amp; echo $PWD; # Return exit 0; Tables    Pages Elements     1 Text   2 Code   3 Tables    </description>
      <content>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#table-of-contents&#34;&gt;Table of contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text&#34;&gt;Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#python&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cc&#34;&gt;C/C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bash&#34;&gt;Bash&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tables&#34;&gt;Tables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;text&#34;&gt;Text&lt;/h2&gt;
&lt;p&gt;Some example text&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;h3 id=&#34;python&#34;&gt;Python&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;foo&lt;/span&gt;():
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;This is a python function&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;cc&#34;&gt;C/C++&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;foo&lt;/span&gt;(){
    prinf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%s&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;This is a C function&amp;#34;&lt;/span&gt;)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;bash&#34;&gt;Bash&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This is a bash command&lt;/span&gt;
cd dir &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; echo $PWD;

&lt;span style=&#34;color:#75715e&#34;&gt;# Return&lt;/span&gt;
exit 0;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;tables&#34;&gt;Tables&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pages&lt;/th&gt;
&lt;th&gt;Elements&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Code&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Tables&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
    </item>
    
  </channel>
</rss>
