<!doctype html><html lang=en><head><title>Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks - Chan et al. :: Roberts blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A better way of doing RAG? I don't think so..."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://www.robert-sokolewicz.nl/reading_papers/2_cag/><link rel=stylesheet href=https://www.robert-sokolewicz.nl/assets/style.css><link rel=stylesheet href=https://www.robert-sokolewicz.nl/assets/css/hugo-cite.css><link rel=apple-touch-icon href=https://www.robert-sokolewicz.nl/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://www.robert-sokolewicz.nl/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks - Chan et al."><meta property="og:description" content="A better way of doing RAG? I don't think so..."><meta property="og:url" content="https://www.robert-sokolewicz.nl/reading_papers/2_cag/"><meta property="og:site_name" content="Roberts blog"><meta property="og:image" content="https://www.robert-sokolewicz.nl"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2025-01-12 00:00:00 +0000 UTC"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],macros:{bb:["{\\boldsymbol{#1}}",1],tr:"{\\DeclareMathOperator{\\tr}{Tr}}",im:"{\\DeclareMathOperator{\\im}{Im}}",re:"{\\DeclareMathOperator{\\re}{Re}}",},processEscapes:true,processEnvironments:true,tags:"ams"},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
    MathJax.Hub.Queue(function() {
        // Fix <code> tags after MathJax finishes running. This is a
        // hack to overcome a shortcoming of Markdown. Discussion at
        // https://github.com/mojombo/jekyll/issues/199
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i &lt; all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Home</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/books>Book Reviews</a></li><li><a href=/cv>CV</a></li><li><a href=/physics>Physics</a></li><li><a href=/reading_papers>Reading Papers</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/books>Book Reviews</a></li><li><a href=/cv>CV</a></li><li><a href=/physics>Physics</a></li><li><a href=/reading_papers>Reading Papers</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://www.robert-sokolewicz.nl/reading_papers/2_cag/>Don&rsquo;t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks - Chan et al.</a></h1><div class=post-meta><span class=post-date>2025-01-12</span>
<span class=post-author>:: Robert</span></div><div class=post-content><div><p>The paper
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#chan2024dontragcacheaugmentedgeneration><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Brian J"><span itemprop=familyName>Chan</span></span> 
<em>& al.</em>, <span itemprop=datePublished>2024</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=default><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chan</span>, <meta itemprop=givenName content="Brian J">B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Chao-Ting">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cheng</span>, <meta itemprop=givenName content="Jui-Hung">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Huang</span>, <meta itemprop=givenName content="Hen-Hsen">H.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name><i>Don’t do RAG: When cache-augmented generation is all you need for knowledge tasks</i></span>.
 Retrieved from 
<a href=https://arxiv.org/abs/2412.15605 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2412.15605</a></span>
</span></span>)</span> presents a &ldquo;novel&rdquo; way of augmenting large language model prompts with relevant information. One way of improving a language model&rsquo;s accuracy and reducing hallucinations is to augment each prompt with relevant information. Because language models have a finite context length, we cannot simply pass all information (e.g. all of Wikipedia) to the model. Instead, often we would implement some sort of retrieval system to fetch relevant pieces of information and passing them to the model.</p><p>The &ldquo;novel&rdquo; approach that is advertised in this paper is to realize that the context length of many models is already sufficiently large to pass all relevant information in one go. For example, right now the context length of GPT-4o is 128k tokens. That would be around 200 pages of text.</p><p>The two immediate issues with this approach are:</p><ol><li>It can be costly to pass all information with each prompt</li><li>Tokenizing a large amount of text can lead to increased latency</li></ol><p>The way around this is to only pass the relevant information to the last prompt and not as part of the chat history. Furthermore, the relevant information can be preprocesed, tokenized and encoded (tokens -> token ids) before hand.</p><p>This approach is called Cache-Augmented Generation (CAG) and the authors claim that this approach is better than RAG, because it is faster and more accurate.</p><p>In my opinion, the proposal is rather trivial and not entirely novel. Also, models tend to get confused when they recieve too much contextual information, so I&rsquo;m skeptical that his approach will work in many cases.</p><p>From a research perspective, I&rsquo;m also a bit critical. Many people have tried alternatives to traditional RAG, so it would be interesting to see how this method compares to others. More importantly, usually in science you follow something similar to the scientific method. Since proving a hypothesis is impossible, but disproving is, typically you try your best to disprove your hypothesis. Failing to do so will give some merit to your original hypothesis. Testing accuracy on a single dataset seems a bit weak in my opinion.</p><p>Lastly, analysis and discussion is rather lacking. It would be interesting to know why and in which cases CAG would outperform RAG. Does the amount of relevant information matter? Does context recall trade off with context precision? Why add information to the prompt, and not to the system prompt?</p><h1 id=bibliography>Bibliography<a href=#bibliography class=hanchor arialabel=Anchor>&#8983;</a></h1><section class=hugo-cite-bibliography><dl><div id=chan2024dontragcacheaugmentedgeneration><dt>Chan, 
Chen, 
Cheng & Huang
(2024)</dt><dd><span itemscope itemtype=https://schema.org/CreativeWork data-type=default><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chan</span>, <meta itemprop=givenName content="Brian J">B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Chao-Ting">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cheng</span>, <meta itemprop=givenName content="Jui-Hung">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Huang</span>, <meta itemprop=givenName content="Hen-Hsen">H.</span>
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name><i>Don’t do RAG: When cache-augmented generation is all you need for knowledge tasks</i></span>.
 Retrieved from 
<a href=https://arxiv.org/abs/2412.15605 itemprop=identifier itemtype=https://schema.org/URL>https://arxiv.org/abs/2412.15605</a></span></dd></div></dl></section></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2025 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://www.robert-sokolewicz.nl/assets/main.js></script><script src=https://www.robert-sokolewicz.nl/assets/prism.js></script></div></body></html>